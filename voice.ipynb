{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YPjIKV2-Vr9",
        "outputId": "676f05af-3a66-4f2f-d23e-6dd2d4a385fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.8)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.11/dist-packages (20240930)\n",
            "Requirement already satisfied: ffmpeg in /usr/local/lib/python3.11/dist-packages (1.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (3.1.6)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (24.1.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.13.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2) (3.0.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.0)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn pyngrok python-multipart \\\n",
        "             openai-whisper ffmpeg numpy \\\n",
        "             jinja2 aiofiles\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update && apt install -y ffmpeg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IC1th4aSAiUN",
        "outputId": "d9401cfd-13ba-477c-9bc9-1bf2bd522bea"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Waiting for headers] [Connected to r2u.stat.illinois.\u001b[0m\r                                                                               \rHit:2 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [Connected to r2u.stat.illinois.edu (192.17.190.167)] \u001b[0m\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "100 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 100 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(\"2xfttaPpiNGf2JQPZmxafQBjRcl_3MxVzmsipcCXzJLimLvca\")\n"
      ],
      "metadata": {
        "id": "PYlH--kFBGLe"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from fastapi import FastAPI, Request, File, UploadFile\n",
        "from fastapi.responses import HTMLResponse, JSONResponse\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from fastapi.templating import Jinja2Templates\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "import whisper\n",
        "import tempfile\n",
        "import os\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# CORS ÏÑ§Ï†ï\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# Ï†ïÏ†Å ÌååÏùº Î∞è ÌÖúÌîåÎ¶ø ÏÑ§Ï†ï\n",
        "app.mount(\"/static\", StaticFiles(directory=\"www\"), name=\"static\")\n",
        "templates = Jinja2Templates(directory=\"www\")\n",
        "\n",
        "# Whisper Î™®Îç∏ Î°úÎìú\n",
        "print(\"üéô Whisper Î™®Îç∏ Î°úÎî© Ï§ë...\")\n",
        "model = whisper.load_model(\"base\")\n",
        "print(\"‚úÖ Whisper Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
        "\n",
        "# Ïù∏Îç±Ïä§ ÌéòÏù¥ÏßÄ\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def index(request: Request):\n",
        "    return templates.TemplateResponse(\"index.html\", {\"request\": request})\n",
        "\n",
        "# Ìä∏ÎûúÏä§ÌÅ¨Î¶ΩÏÖò API\n",
        "@app.post(\"/transcribe\")\n",
        "async def transcribe(file: UploadFile = File(...)):\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.webm') as temp_file:\n",
        "        audio_data = await file.read()\n",
        "        temp_file.write(audio_data)\n",
        "        temp_file_path = temp_file.name\n",
        "\n",
        "    try:\n",
        "        result = model.transcribe(temp_file_path)\n",
        "        os.unlink(temp_file_path)\n",
        "        return JSONResponse(content={\"text\": result[\"text\"]})\n",
        "    except Exception as e:\n",
        "        try:\n",
        "            os.unlink(temp_file_path)\n",
        "        except:\n",
        "            pass\n",
        "        print(f\"‚ùå Ïò§Î•ò Î∞úÏÉù: {e}\")\n",
        "        return JSONResponse(content={\"error\": str(e)}, status_code=500)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=3000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7az3DDRFBliW",
        "outputId": "6bf6dd51-b030-4b9b-c11f-2366b311c34b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dirs = [\"www\"]\n",
        "for d in dirs:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "    print(f\"‚úÖ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±Îê®: {d}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lltbLmDBDCaB",
        "outputId": "c14af949-d943-4180-9a14-a2eca740037c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ ÎîîÎ†âÌÜ†Î¶¨ ÏÉùÏÑ±Îê®: www\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile www/index.html\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"ko\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <title>Whisper ÏùåÏÑ± Ïù∏Ïãù Îç∞Î™®</title>\n",
        "  <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css?family=Roboto:300,400,500,700&display=swap\"/>\n",
        "  <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\"/>\n",
        "  <script src=\"https://unpkg.com/react@18/umd/react.development.js\"></script>\n",
        "  <script src=\"https://unpkg.com/react-dom@18/umd/react-dom.development.js\"></script>\n",
        "  <script src=\"https://unpkg.com/@mui/material@5.14.5/umd/material-ui.development.js\"></script>\n",
        "  <script src=\"https://unpkg.com/@babel/standalone/babel.min.js\"></script>\n",
        "  <style>\n",
        "    body { margin: 0; font-family: 'Roboto', sans-serif; }\n",
        "    .container { max-width: 800px; margin: 0 auto; padding: 20px; }\n",
        "    .header { background-color: #1976d2; color: white; padding: 16px; margin-bottom: 20px; }\n",
        "    .chat-bubble { background-color: #f5f5f5; padding: 16px; margin-bottom: 8px; border-radius: 4px; }\n",
        "    .transcription-container { max-height: 300px; overflow-y: auto; margin-top: 16px; }\n",
        "    .button-container { display: flex; gap: 8px; margin-top: 16px; }\n",
        "    .url-input { width: 100%; margin-bottom: 16px; }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <div id=\"root\"></div>\n",
        "\n",
        "  <script type=\"text/babel\">\n",
        "    const { useState, useRef, useEffect } = React;\n",
        "    const { AppBar, Toolbar, Typography, Button, Container, Box, TextField } = MaterialUI;\n",
        "\n",
        "    const App = () => {\n",
        "      const [isRecording, setIsRecording] = useState(false);\n",
        "      const [transcriptions, setTranscriptions] = useState([]);\n",
        "      const [apiUrl, setApiUrl] = useState('');\n",
        "      const mediaRecorderRef = useRef(null);\n",
        "      const audioChunksRef = useRef([]);\n",
        "\n",
        "      useEffect(() => {\n",
        "        const protocol = window.location.protocol;\n",
        "        const hostname = window.location.hostname;\n",
        "        const port = window.location.port;\n",
        "        const baseUrl = `${protocol}//${hostname}${port ? ':' + port : ''}`;\n",
        "        setApiUrl(`${baseUrl}/transcribe`);\n",
        "      }, []);\n",
        "\n",
        "      const handleApiUrlChange = (event) => {\n",
        "        setApiUrl(event.target.value);\n",
        "      };\n",
        "\n",
        "      const handleStartRecording = async () => {\n",
        "        try {\n",
        "          const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "          mediaRecorderRef.current = new MediaRecorder(stream);\n",
        "          mediaRecorderRef.current.ondataavailable = (event) => {\n",
        "            audioChunksRef.current.push(event.data);\n",
        "          };\n",
        "          mediaRecorderRef.current.onstop = () => {\n",
        "            sendAudioData();\n",
        "          };\n",
        "          audioChunksRef.current = [];\n",
        "          mediaRecorderRef.current.start();\n",
        "          setIsRecording(true);\n",
        "        } catch (error) {\n",
        "          console.error('Error accessing microphone:', error);\n",
        "          alert('ÎßàÏù¥ÌÅ¨ Ï†ëÍ∑ºÏóê Ïã§Ìå®ÌñàÏäµÎãàÎã§.');\n",
        "        }\n",
        "      };\n",
        "\n",
        "      const handleStopRecording = () => {\n",
        "        if (mediaRecorderRef.current && isRecording) {\n",
        "          mediaRecorderRef.current.stop();\n",
        "          setIsRecording(false);\n",
        "        }\n",
        "      };\n",
        "\n",
        "      const sendAudioData = async () => {\n",
        "        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/webm' });\n",
        "        const formData = new FormData();\n",
        "        formData.append('file', audioBlob, 'recording.webm');\n",
        "\n",
        "        try {\n",
        "          const response = await fetch(apiUrl, {\n",
        "            method: 'POST',\n",
        "            body: formData,\n",
        "          });\n",
        "          const data = await response.json();\n",
        "          if (data.text) {\n",
        "            setTranscriptions((prev) => [...prev, data.text]);\n",
        "          } else if (data.error) {\n",
        "            setTranscriptions((prev) => [...prev, `Error: ${data.error}`]);\n",
        "          }\n",
        "        } catch (error) {\n",
        "          console.error('Error sending audio data:', error);\n",
        "          setTranscriptions((prev) => [...prev, `Error: ${error.message}`]);\n",
        "        }\n",
        "      };\n",
        "\n",
        "      return (\n",
        "        <Container className=\"container\">\n",
        "          <div className=\"header\">\n",
        "            <Typography variant=\"h6\">üéô Whisper ÏùåÏÑ± Ïù∏Ïãù Îç∞Î™®</Typography>\n",
        "          </div>\n",
        "          <Box>\n",
        "            <TextField\n",
        "              label=\"API URL\"\n",
        "              variant=\"outlined\"\n",
        "              fullWidth\n",
        "              value={apiUrl}\n",
        "              onChange={handleApiUrlChange}\n",
        "              className=\"url-input\"\n",
        "            />\n",
        "            <div className=\"button-container\">\n",
        "              <Button\n",
        "                variant=\"contained\"\n",
        "                color=\"primary\"\n",
        "                onClick={handleStartRecording}\n",
        "                disabled={isRecording}\n",
        "              >\n",
        "                ÎÖπÏùå ÏãúÏûë\n",
        "              </Button>\n",
        "              <Button\n",
        "                variant=\"contained\"\n",
        "                color=\"secondary\"\n",
        "                onClick={handleStopRecording}\n",
        "                disabled={!isRecording}\n",
        "              >\n",
        "                ÎÖπÏùå Ï¢ÖÎ£å\n",
        "              </Button>\n",
        "            </div>\n",
        "          </Box>\n",
        "          <div className=\"transcription-container\">\n",
        "            {transcriptions.map((text, index) => (\n",
        "              <div key={index} className=\"chat-bubble\">\n",
        "                {text}\n",
        "              </div>\n",
        "            ))}\n",
        "          </div>\n",
        "        </Container>\n",
        "      );\n",
        "    };\n",
        "\n",
        "    ReactDOM.render(<App />, document.getElementById('root'));\n",
        "  </script>\n",
        "</body>\n",
        "</html>\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Qm4102CXBV",
        "outputId": "68167df0-b675-42b4-950f-fb931dc2c8c2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting www/index.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run_server.py\n",
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "def run():\n",
        "    subprocess.run([\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"3000\"])\n",
        "\n",
        "thread = threading.Thread(target=run)\n",
        "thread.daemon = True\n",
        "thread.start()\n",
        "\n",
        "time.sleep(3)\n",
        "\n",
        "public_url = ngrok.connect(3000).public_url\n",
        "print(\"üîó Ï†ëÏÜç Ï£ºÏÜå:\", public_url)\n",
        "\n",
        "# ÏÑúÎ≤ÑÎ•º Í≥ÑÏÜç Ïã§Ìñâ\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"‚õî ÏÑúÎ≤Ñ Ï¢ÖÎ£å\")\n",
        "    ngrok.kill()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W5blPTegChXI",
        "outputId": "3080b496-513b-42cd-99b8-03a2b261ebaf"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run_server.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run_server.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcLRX5m0DQ6O",
        "outputId": "011e9bc5-3522-486c-d2d5-62e0f9bcbbb5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéô Whisper Î™®Îç∏ Î°úÎî© Ï§ë...\n",
            "üîó Ï†ëÏÜç Ï£ºÏÜå: https://823c-34-106-193-135.ngrok-free.app\n",
            "‚úÖ Whisper Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\n",
            "\u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m23527\u001b[0m]\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[32mINFO\u001b[0m:     Uvicorn running on \u001b[1mhttp://0.0.0.0:3000\u001b[0m (Press CTRL+C to quit)\n",
            "\u001b[32mINFO\u001b[0m:     27.35.110.203:0 - \"\u001b[1mGET / HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[32mINFO\u001b[0m:     27.35.110.203:0 - \"\u001b[1mGET /favicon.ico HTTP/1.1\u001b[0m\" \u001b[31m404 Not Found\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\u001b[32mINFO\u001b[0m:     27.35.110.203:0 - \"\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "\u001b[32mINFO\u001b[0m:     27.35.110.203:0 - \"\u001b[1mPOST /transcribe HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run_server.py\", line 21, in <module>\n",
            "    time.sleep(1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/run_server.py\", line 23, in <module>\n",
            "    print(\"‚õî ÏÑúÎ≤Ñ Ï¢ÖÎ£å\")\n",
            "KeyboardInterrupt\n",
            "\u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[32mINFO\u001b[0m:     Application shutdown complete.\n",
            "\u001b[32mINFO\u001b[0m:     Finished server process [\u001b[36m23527\u001b[0m]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VWmd_gBCDVgV"
      },
      "execution_count": 41,
      "outputs": []
    }
  ]
}